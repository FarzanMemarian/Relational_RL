{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "class Gridworld: # Environment\n",
    "    def __init__(self, n_dim, start, n_obj, min_num, max_num):\n",
    "        # creates a square gridworld\n",
    "        self.width = n_dim\n",
    "        self.height = n_dim\n",
    "        self.n_dim = n_dim\n",
    "        self.n_obj = n_obj\n",
    "        self.min_num = min_num\n",
    "        self.max_num = max_num\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "        self.grid_mat = np.zeros((self.height, self.width),dtype=int)\n",
    "        \n",
    "        # call functions\n",
    "        self.original_objects = self.create_objects()\n",
    "        self.place_objects()\n",
    "        self.current_objects = copy.deepcopy(self.original_objects)\n",
    "        self.actions = {}\n",
    "        self.set_actions()\n",
    "\n",
    "    def set_actions(self):\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        for i in range(self.n_dim):\n",
    "            for j in range(self.n_dim):\n",
    "                if i != 0 and i != self.n_dim-1 and j != 0 and j != self.n_dim-1:\n",
    "                    self.actions[(i,j)] = ['U','D','R','L']\n",
    "                else:\n",
    "                    if i == 0 and j != 0 and j != self.n_dim-1:\n",
    "                        self.actions[(i,j)] = ['D','R','L']\n",
    "                    if i == self.n_dim-1 and j != 0 and j != self.n_dim-1:\n",
    "                        self.actions[(i,j)] = ['U','R','L']\n",
    "                    if j == 0 and i != 0 and i != self.n_dim-1:\n",
    "                        self.actions[(i,j)] = ['U','D','R']\n",
    "                    if j == self.n_dim-1 and i != 0 and i != self.n_dim-1:\n",
    "                        self.actions[(i,j)] = ['U','D','L']\n",
    "                    if i == 0 and j == 0:\n",
    "                        self.actions[(i,j)] = ['D','R']\n",
    "                    if i == self.n_dim-1 and j == 0:\n",
    "                        self.actions[(i,j)] = ['U','R']\n",
    "                    if j == self.n_dim-1 and i == 0:\n",
    "                        self.actions[(i,j)] = ['D','L']\n",
    "                    if j == self.n_dim-1 and i == self.n_dim-1:\n",
    "                        self.actions[(i,j)] = ['U','L']\n",
    "\n",
    "    def rewards(self,i,j):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        element = self.grid_mat[i,j]\n",
    "        if element == self.current_objects[-1]:\n",
    "            self.current_objects.pop(-1)\n",
    "            if len(self.current_objects) >= 1:\n",
    "                reward = 100\n",
    "            if len(self.current_objects) == 0:\n",
    "                reward = 10000\n",
    "            \n",
    "        elif element != 0 and element != self.current_objects[-1]:\n",
    "            reward = -1000\n",
    "        elif element == 0:\n",
    "            reward = -1\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    def print_grid(self):\n",
    "        print (self.grid_mat)\n",
    "    \n",
    "    def place_objects(self):\n",
    "        small_matrix_dim = (self.n_dim+1)/2\n",
    "        a = np.arange(0,small_matrix_dim**2, dtype=int)\n",
    "        idx_1d = np.random.choice(a, size=self.n_obj, replace=False, p=None)\n",
    "        idx_2d = [[2*math.floor(idx/small_matrix_dim), 2*int(idx%small_matrix_dim)] for idx in idx_1d]\n",
    "        for counter, idx in enumerate(idx_2d):\n",
    "            self.grid_mat[idx[0], idx[1]] = self.original_objects[counter]\n",
    "\n",
    "    def create_objects(self):\n",
    "        a = np.arange(self.min_num, self.max_num+1)\n",
    "        objects = np.random.choice(a, size=self.n_obj, replace=False, p=None)\n",
    "        objects = list(objects)\n",
    "        objects.sort(reverse=True)\n",
    "        return objects\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def take_action(self, action):\n",
    "    # check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return a reward (if any)\n",
    "        return self.rewards(self.i, self.j), (self.i, self.j)\n",
    "\n",
    "    def undo_move(self, action):\n",
    "    # these are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    def game_over(self):\n",
    "        # returns true if game is over, else false\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    def all_states(self):\n",
    "        return set(self.actions.keys() + self.rewards.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78  0 77  0  0  0  0  0 95]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 56  0  0  0 73]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [11  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0]\n",
      " [97  0  0  0 69  0  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "# n_dim should be an odd number\n",
    "grid_w = Gridworld(n_dim= 9, start=[0,1], n_obj=10, min_num=1, max_num=100)\n",
    "grid_w.print_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta controller operates using Q-learning and relational networks\n",
    "class Meta_controller:\n",
    "    def __init__(self, goals):\n",
    "        self.DM = [] \n",
    "        self.goals = goals\n",
    "        \n",
    "\n",
    "class Controller:\n",
    "    def __init__(self,g):\n",
    "        self.DC = []\n",
    "        self.current_goal = g\n",
    "        \n",
    "        \n",
    "def EpsGreedy(x, beta, eps, Q):\n",
    "    if np.random.rand() < eps:\n",
    "        return np.random.choice(np.arange(len(beta)), size=1)\n",
    "    else:\n",
    "        idx = np.argmax[Q(x,m) for m in beta]\n",
    "        return beta(idx)\n",
    "    \n",
    "\n",
    "def UpdateParams(L,D,mini_size):\n",
    "    idxs = np.random.choice(len(D), size = mini_size)\n",
    "    examples = D[idxs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = 10\n",
    "# initialize the grid_w\n",
    "grid_w = Gridworld(n_dim= 9, start=[0,1], n_obj=10, min_num=1, max_num=100)\n",
    "goals = grid_w.original_objects\n",
    "eps1 = np.ones((n_obj,1))\n",
    "eps2 = 1\n",
    "meta_controller = meta_controller(grid_w.original_objects)\n",
    "\n",
    "num_epis = 1000\n",
    "for i in range(num_epis):\n",
    "    s = [0,1]\n",
    "    g_idx = epsGreedy(s, goals, eps2, Q2)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        g = goals[g_idx]\n",
    "        controller = Controller(g)\n",
    "        F = 0\n",
    "        s0 = s\n",
    "        while True: #(s is terminal or goal g reached)\n",
    "            a_idx = EpsGreedy((s,g),grid_w.actions[s],eps1[g_idx])\n",
    "            a = grid_w.actions[s][a_idx]\n",
    "            f, s_prime = grid_w.take_action(a)\n",
    "            r = f\n",
    "            # obtain intrinsic reward for the controller\n",
    "            controller.DC.append([(s,g),a,r,(s_prime,g)])\n",
    "            UpdateParams(L1,controller.DC)\n",
    "            UpdateParams(L2,meta_controller.DM)\n",
    "            F += f\n",
    "            s = s_prime\n",
    "        meta_controller.DM.append((s0,g,F,s_prime))\n",
    "        if s not terminal:\n",
    "            g_idx = epsGreedy(s, goals, eps2, Q2)\n",
    "    eps1 *= 0.99\n",
    "    eps2 *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/EthanMacdonald/h-DQN/blob/master/agent/hDQN.py\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "# Default architecture for the meta controller\n",
    "default_meta_layers = [Dense] * 5\n",
    "default_meta_inits = ['lecun_uniform', 'lecun_uniform', 'lecun_uniform', 'lecun_uniform', 'lecun_uniform']\n",
    "default_meta_nodes = [6, 30, 30, 30, 6]\n",
    "default_meta_activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
    "default_meta_loss = \"mean_squared_error\"\n",
    "default_meta_optimizer=RMSprop(lr=0.00025, rho=0.9, epsilon=1e-06)\n",
    "default_meta_n_samples = 1000\n",
    "default_meta_epsilon = 1.0;\n",
    "\n",
    "# Default architectures for the lower level controller/actor\n",
    "default_layers = [Dense] * 5\n",
    "default_inits = ['lecun_uniform'] * 5\n",
    "default_nodes = [12, 30, 30, 30, 2]\n",
    "default_activations = ['relu'] * 5\n",
    "default_loss = \"mean_squared_error\"\n",
    "default_optimizer=RMSprop(lr=0.00025, rho=0.9, epsilon=1e-06)\n",
    "default_n_samples = 1000\n",
    "default_gamma = 0.975\n",
    "default_epsilon = 1.0\n",
    "default_actor_epsilon = [1.0]*6\n",
    "default_tau = 0.001\n",
    "\n",
    "class hDQN:\n",
    "\n",
    "    def __init__(self, meta_layers=default_meta_layers, meta_inits=default_meta_inits,\n",
    "                meta_nodes=default_meta_nodes, meta_activations=default_meta_activations,\n",
    "                meta_loss=default_meta_loss, meta_optimizer=default_meta_optimizer,\n",
    "                layers=default_layers, inits=default_inits, nodes=default_nodes,\n",
    "                activations=default_activations, loss=default_loss,\n",
    "                optimizer=default_optimizer, n_samples=default_n_samples,\n",
    "                meta_n_samples=default_meta_n_samples, gamma=default_gamma,\n",
    "                meta_epsilon=default_meta_epsilon, epsilon=default_epsilon, actor_epsilon = default_actor_epsilon, tau = default_tau):\n",
    "        self.meta_layers = meta_layers\n",
    "        self.meta_inits = meta_inits\n",
    "        self.meta_nodes = meta_nodes\n",
    "        self.meta_activations = meta_activations\n",
    "        self.meta_loss = meta_loss\n",
    "        self.meta_optimizer = meta_optimizer\n",
    "        self.layers = layers\n",
    "        self.inits = inits\n",
    "        self.nodes = nodes\n",
    "        self.activations = activations\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.meta_controller = self.meta_controller()\n",
    "        self.target_meta_controller = self.target_meta_controller()\n",
    "        self.actor = self.actor()\n",
    "        self.target_actor = self.target_actor()\n",
    "        self.goal_selected = np.ones(6)\n",
    "        self.goal_success = np.zeros(6)\n",
    "        self.meta_epsilon = meta_epsilon\n",
    "        self.actor_epsilon = actor_epsilon\n",
    "        self.n_samples = n_samples\n",
    "        self.meta_n_samples = meta_n_samples\n",
    "        self.gamma = gamma\n",
    "        self.target_tau = tau\n",
    "        self.memory = []\n",
    "        self.meta_memory = []\n",
    "\n",
    "    def meta_controller(self):\n",
    "        meta = Sequential()\n",
    "        meta.add(self.meta_layers[0](self.meta_nodes[0], init=self.meta_inits[0], input_shape=(self.meta_nodes[0],)))\n",
    "        meta.add(Activation(self.meta_activations[0]))\n",
    "        for layer, init, node, activation in list(zip(self.meta_layers, self.meta_inits, self.meta_nodes, self.meta_activations))[1:]:\n",
    "            meta.add(layer(node, init=init, input_shape=(node,)))\n",
    "            meta.add(Activation(activation))\n",
    "            print(\"meta node: \" + str(node))\n",
    "        meta.compile(loss=self.meta_loss, optimizer=self.meta_optimizer)\n",
    "        return meta\n",
    "    \n",
    "    def target_meta_controller(self):\n",
    "        meta = Sequential()\n",
    "        meta.add(self.meta_layers[0](self.meta_nodes[0], init=self.meta_inits[0], input_shape=(self.meta_nodes[0],)))\n",
    "        meta.add(Activation(self.meta_activations[0]))\n",
    "        for layer, init, node, activation in list(zip(self.meta_layers, self.meta_inits, self.meta_nodes, self.meta_activations))[1:]:\n",
    "            meta.add(layer(node, init=init, input_shape=(node,)))\n",
    "            meta.add(Activation(activation))\n",
    "            print(\"meta node: \" + str(node))\n",
    "        meta.compile(loss=self.meta_loss, optimizer=self.meta_optimizer)\n",
    "        return meta\n",
    "\n",
    "\n",
    "    def actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(self.layers[0](self.nodes[0], init=self.inits[0], input_shape=(self.nodes[0],)))\n",
    "        actor.add(Activation(self.activations[0]))\n",
    "        for layer, init, node, activation in list(zip(self.layers, self.inits, self.nodes, self.activations))[1:]:\n",
    "            print(node)\n",
    "            actor.add(layer(node, init=init, input_shape=(node,)))\n",
    "            actor.add(Activation(activation))\n",
    "        actor.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        return actor\n",
    "    \n",
    "    def target_actor(self):\n",
    "        actor = Sequential()\n",
    "        actor.add(self.layers[0](self.nodes[0], init=self.inits[0], input_shape=(self.nodes[0],)))\n",
    "        actor.add(Activation(self.activations[0]))\n",
    "        for layer, init, node, activation in list(zip(self.layers, self.inits, self.nodes, self.activations))[1:]:\n",
    "            print(node)\n",
    "            actor.add(layer(node, init=init, input_shape=(node,)))\n",
    "            actor.add(Activation(activation))\n",
    "        actor.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        return actor\n",
    "\n",
    "    def select_move(self, state, goal, goal_value):\n",
    "        vector = np.concatenate([state, goal], axis=1)\n",
    "        if random.random() < self.actor_epsilon[goal_value-1]:\n",
    "            return np.argmax(self.actor.predict(vector, verbose=0))\n",
    "        return random.choice([0,1])\n",
    "\n",
    "    def select_goal(self, state):\n",
    "        if self.meta_epsilon < random.random():\n",
    "            pred = self.meta_controller.predict(state, verbose=0)\n",
    "            print(\"pred shape: \" + str(pred.shape))\n",
    "            return np.argmax(pred)+1\n",
    "        print(\"Exploring\");\n",
    "        return random.choice([1,2,3,4,5,6])\n",
    "\n",
    "    def criticize(self, goal, next_state):\n",
    "        return 1.0 if goal == next_state else 0.0\n",
    "\n",
    "    def store(self, experience, meta=False):\n",
    "        if meta:\n",
    "            self.meta_memory.append(experience)\n",
    "            if len(self.meta_memory) > 1000000:\n",
    "                self.meta_memory = self.meta_memory[-100:]\n",
    "        else:\n",
    "            self.memory.append(experience)\n",
    "            if len(self.memory) > 1000000:\n",
    "                self.memory = self.memory[-1000000:]\n",
    "\n",
    "    def _update(self):\n",
    "        exps = [random.choice(self.memory) for _ in range(self.n_samples)]\n",
    "        state_vectors = np.squeeze(np.asarray([np.concatenate([exp.state, exp.goal], axis=1) for exp in exps]))\n",
    "        next_state_vectors = np.squeeze(np.asarray([np.concatenate([exp.next_state, exp.goal], axis=1) for exp in exps]))\n",
    "        try:\n",
    "            reward_vectors = self.actor.predict(state_vectors, verbose=0)\n",
    "        except Exception as e:\n",
    "            state_vectors = np.expand_dims(state_vectors, axis=0)\n",
    "            reward_vectors = self.actor.predict(state_vectors, verbose=0)\n",
    "        \n",
    "        try:\n",
    "            next_state_reward_vectors = self.target_actor.predict(next_state_vectors, verbose=0)\n",
    "        except Exception as e:\n",
    "            next_state_vectors = np.expand_dims(next_state_vectors, axis=0)\n",
    "            next_state_reward_vectors = self.target_actor.predict(next_state_vectors, verbose=0)\n",
    "        \n",
    "        for i, exp in enumerate(exps):\n",
    "            reward_vectors[i][exp.action] = exp.reward\n",
    "            if not exp.done:\n",
    "                reward_vectors[i][exp.action] += self.gamma * max(next_state_reward_vectors[i])\n",
    "        reward_vectors = np.asarray(reward_vectors)\n",
    "        self.actor.fit(state_vectors, reward_vectors, verbose=0)\n",
    "        \n",
    "        #Update target network\n",
    "        actor_weights = self.actor.get_weights()\n",
    "        actor_target_weights = self.target_actor.get_weights()\n",
    "        for i in range(len(actor_weights)):\n",
    "            actor_target_weights[i] = self.target_tau * actor_weights[i] + (1 - self.target_tau) * actor_target_weights[i]\n",
    "        self.target_actor.set_weights(actor_target_weights)\n",
    "\n",
    "    def _update_meta(self):\n",
    "        if 0 < len(self.meta_memory):\n",
    "            exps = [random.choice(self.meta_memory) for _ in range(self.meta_n_samples)]\n",
    "            state_vectors = np.squeeze(np.asarray([exp.state for exp in exps]))\n",
    "            next_state_vectors = np.squeeze(np.asarray([exp.next_state for exp in exps]))\n",
    "            try:\n",
    "                reward_vectors = self.meta_controller.predict(state_vectors, verbose=0)\n",
    "            except Exception as e:\n",
    "                state_vectors = np.expand_dims(state_vectors, axis=0)\n",
    "                reward_vectors = self.meta_controller.predict(state_vectors, verbose=0)\n",
    "            \n",
    "            try:\n",
    "                next_state_reward_vectors = self.target_meta_controller.predict(next_state_vectors, verbose=0)\n",
    "            except Exception as e:\n",
    "                next_state_vectors = np.expand_dims(next_state_vectors, axis=0)\n",
    "                next_state_reward_vectors = self.target_meta_controller.predict(next_state_vectors, verbose=0)\n",
    "            \n",
    "            for i, exp in enumerate(exps):\n",
    "                reward_vectors[i][np.argmax(exp.goal)] = exp.reward\n",
    "                if not exp.done:\n",
    "                    reward_vectors[i][np.argmax(exp.goal)] += self.gamma * max(next_state_reward_vectors[i])\n",
    "            self.meta_controller.fit(state_vectors, reward_vectors, verbose=0)\n",
    "            \n",
    "            #Update target network\n",
    "            meta_weights = self.meta_controller.get_weights()\n",
    "            meta_target_weights = self.target_meta_controller.get_weights()\n",
    "            for i in range(len(meta_weights)):\n",
    "                meta_target_weights[i] = self.target_tau * meta_weights[i] + (1 - self.target_tau) * meta_target_weights[i]\n",
    "            self.target_meta_controller.set_weights(meta_target_weights)\n",
    "\n",
    "    def update(self, meta=False):\n",
    "        if meta:\n",
    "            self._update_meta()\n",
    "        else:\n",
    "            self._update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
